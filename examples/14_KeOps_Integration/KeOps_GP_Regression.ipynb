{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPyTorch Regression With KeOps\n",
    "\n",
    "## Introduction\n",
    "\n",
    "`KeOps` (https://github.com/getkeops/keops) is a recently released software package for fast kernel operations that integrates wih PyTorch. We can use the ability of `KeOps` to perform efficient kernel matrix multiplies on the GPU to integrate with the rest of GPyTorch.\n",
    "\n",
    "In this tutorial, we'll demonstrate how to integrate the kernel matmuls of `KeOps` with all of the bells of whistles of GPyTorch, including things like our preconditioning for conjugate gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %set_env CUDA_VISIBLE_DEVICES=1\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Data\n",
    "We will be using the Protein UCI dataset which contains a total of 40000+ data points. The next cell will download this dataset from a Google drive and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from scipy.io import loadmat\n",
    "dataset = 'protein'\n",
    "if not os.path.isfile(f'{dataset}.mat'):\n",
    "    print(f'Downloading \\'{dataset}\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1nRb8e7qooozXkNghC5eQS0JeywSXGX2S',\n",
    "                               f'{dataset}.mat')\n",
    "    \n",
    "data = torch.Tensor(loadmat(f'{dataset}.mat')['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = data.shape[0]\n",
    "# make train/val/test\n",
    "n_train = int(0.8 * N)\n",
    "train_x, train_y = data[:n_train, :-1], data[:n_train, -1]\n",
    "test_x, test_y = data[n_train:, :-1], data[n_train:, -1]\n",
    "\n",
    "# normalize features\n",
    "mean = train_x.mean(dim=-2, keepdim=True)\n",
    "std = train_x.std(dim=-2, keepdim=True) + 1e-6 # prevent dividing by 0\n",
    "train_x = (train_x - mean) / std\n",
    "test_x = (test_x - mean) / std\n",
    "\n",
    "# normalize labels\n",
    "mean, std = train_y.mean(),train_y.std()\n",
    "train_y = (train_y - mean) / std\n",
    "test_y = (test_y - mean) / std\n",
    "\n",
    "# make continguous\n",
    "train_x, train_y = train_x.contiguous(), train_y.contiguous()\n",
    "test_x, test_y = test_x.contiguous(), test_y.contiguous()\n",
    "\n",
    "output_device = torch.device('cuda:0')\n",
    "\n",
    "train_x, train_y = train_x.to(output_device), train_y.to(output_device)\n",
    "test_x, test_y = test_x.to(output_device), test_y.to(output_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "\n",
    "Using KeOps with one of our pre built kernels is as straightforward as swapping the kernel out. For example, in the cell below, we copy the simple GP from our basic tutorial notebook, and swap out `gpytorch.kernels.MaternKernel` for `gpytorch.kernels.keops.MaternKernel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.keops.MaternKernel(nu=2.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()\n",
    "model = ExactGPModel(train_x, train_y, likelihood).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 0.589   lengthscale: 3.033   noise: 0.089\n",
      "0.5267367362976074\n",
      "Iter 2/50 - Loss: 0.598   lengthscale: 3.129   noise: 0.081\n",
      "0.5038349628448486\n",
      "Iter 3/50 - Loss: 0.571   lengthscale: 3.220   noise: 0.082\n",
      "0.4416794776916504\n",
      "Iter 4/50 - Loss: 0.559   lengthscale: 3.305   noise: 0.084\n",
      "0.4224965572357178\n",
      "Iter 5/50 - Loss: 0.546   lengthscale: 3.386   noise: 0.088\n",
      "0.41598010063171387\n",
      "Iter 6/50 - Loss: 0.536   lengthscale: 3.461   noise: 0.092\n",
      "0.4090542793273926\n",
      "Iter 7/50 - Loss: 0.516   lengthscale: 3.533   noise: 0.096\n",
      "0.3613882064819336\n",
      "Iter 8/50 - Loss: 0.509   lengthscale: 3.601   noise: 0.096\n",
      "0.3575630187988281\n",
      "Iter 9/50 - Loss: 0.508   lengthscale: 3.664   noise: 0.094\n",
      "0.3791520595550537\n",
      "Iter 10/50 - Loss: 0.522   lengthscale: 3.719   noise: 0.091\n",
      "0.38656091690063477\n",
      "Iter 11/50 - Loss: 0.508   lengthscale: 3.766   noise: 0.088\n",
      "0.39499783515930176\n",
      "Iter 12/50 - Loss: 0.518   lengthscale: 3.796   noise: 0.085\n",
      "0.40425920486450195\n",
      "Iter 13/50 - Loss: 0.525   lengthscale: 3.812   noise: 0.084\n",
      "0.4158463478088379\n",
      "Iter 14/50 - Loss: 0.521   lengthscale: 3.815   noise: 0.083\n",
      "0.4932892322540283\n",
      "Iter 15/50 - Loss: 0.543   lengthscale: 3.807   noise: 0.083\n",
      "0.4583451747894287\n",
      "Iter 16/50 - Loss: 0.548   lengthscale: 3.796   noise: 0.085\n",
      "0.46480274200439453\n",
      "Iter 17/50 - Loss: 0.540   lengthscale: 3.785   noise: 0.087\n",
      "0.51192307472229\n",
      "Iter 18/50 - Loss: 0.546   lengthscale: 3.776   noise: 0.090\n",
      "0.4682350158691406\n",
      "Iter 19/50 - Loss: 0.556   lengthscale: 3.772   noise: 0.092\n",
      "0.46701955795288086\n",
      "Iter 20/50 - Loss: 0.542   lengthscale: 3.779   noise: 0.094\n",
      "0.43764376640319824\n",
      "Iter 21/50 - Loss: 0.551   lengthscale: 3.794   noise: 0.095\n",
      "0.4625883102416992\n",
      "Iter 22/50 - Loss: 0.532   lengthscale: 3.820   noise: 0.094\n",
      "0.4164125919342041\n",
      "Iter 23/50 - Loss: 0.557   lengthscale: 3.851   noise: 0.092\n",
      "0.4912734031677246\n",
      "Iter 24/50 - Loss: 0.555   lengthscale: 3.887   noise: 0.089\n",
      "0.5559864044189453\n",
      "Iter 25/50 - Loss: 0.559   lengthscale: 3.927   noise: 0.087\n",
      "0.5327601432800293\n",
      "Iter 26/50 - Loss: 0.543   lengthscale: 3.966   noise: 0.085\n",
      "0.5146927833557129\n",
      "Iter 27/50 - Loss: 0.537   lengthscale: 4.000   noise: 0.084\n",
      "0.43804359436035156\n",
      "Iter 28/50 - Loss: 0.536   lengthscale: 4.029   noise: 0.082\n",
      "0.4342367649078369\n",
      "Iter 29/50 - Loss: 0.554   lengthscale: 4.051   noise: 0.081\n",
      "0.4643881320953369\n",
      "Iter 30/50 - Loss: 0.548   lengthscale: 4.067   noise: 0.081\n",
      "0.44389867782592773\n",
      "Iter 31/50 - Loss: 0.530   lengthscale: 4.076   noise: 0.082\n",
      "0.40947985649108887\n",
      "Iter 32/50 - Loss: 0.564   lengthscale: 4.076   noise: 0.081\n",
      "0.5228478908538818\n",
      "Iter 33/50 - Loss: 0.559   lengthscale: 4.074   noise: 0.082\n",
      "0.5151913166046143\n",
      "Iter 34/50 - Loss: 0.569   lengthscale: 4.072   noise: 0.084\n",
      "0.5408072471618652\n",
      "Iter 35/50 - Loss: 0.541   lengthscale: 4.075   noise: 0.087\n",
      "0.4760885238647461\n",
      "Iter 36/50 - Loss: 0.548   lengthscale: 4.079   noise: 0.088\n",
      "0.46231985092163086\n",
      "Iter 37/50 - Loss: 0.523   lengthscale: 4.088   noise: 0.089\n",
      "0.4274454116821289\n",
      "Iter 38/50 - Loss: 0.557   lengthscale: 4.101   noise: 0.087\n",
      "0.4952268600463867\n",
      "Iter 39/50 - Loss: 0.521   lengthscale: 4.118   noise: 0.086\n",
      "0.41435694694519043\n",
      "Iter 40/50 - Loss: 0.559   lengthscale: 4.132   noise: 0.082\n",
      "0.4613039493560791\n",
      "Iter 41/50 - Loss: 0.498   lengthscale: 4.144   noise: 0.080\n",
      "0.37143421173095703\n",
      "Iter 42/50 - Loss: 0.574   lengthscale: 4.157   noise: 0.075\n",
      "0.4642205238342285\n",
      "Iter 43/50 - Loss: 0.603   lengthscale: 4.161   noise: 0.073\n",
      "0.5190582275390625\n",
      "Iter 44/50 - Loss: 0.609   lengthscale: 4.157   noise: 0.072\n",
      "0.5418822765350342\n",
      "Iter 45/50 - Loss: 0.598   lengthscale: 4.147   noise: 0.073\n",
      "0.5297873020172119\n",
      "Iter 46/50 - Loss: 0.609   lengthscale: 4.133   noise: 0.075\n",
      "0.5633969306945801\n",
      "Iter 47/50 - Loss: 0.555   lengthscale: 4.123   noise: 0.079\n",
      "0.4395630359649658\n",
      "Iter 48/50 - Loss: 0.596   lengthscale: 4.112   noise: 0.082\n",
      "0.5334904193878174\n",
      "Iter 49/50 - Loss: 0.577   lengthscale: 4.112   noise: 0.085\n",
      "0.4734311103820801\n",
      "Iter 50/50 - Loss: 0.587   lengthscale: 4.122   noise: 0.087\n",
      "0.5158843994140625\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "import time\n",
    "training_iter = 50\n",
    "for i in range(training_iter):\n",
    "    start_time = time.time()\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()\n",
    "    print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling libKeOpstorchd7ba409487 in /home/jake.gardner/.cache/pykeops-1.1.1//build-libKeOpstorchd7ba409487:\n",
      "       formula: Sum_Reduction(((((Var(0,1,2) * Sqrt(Sum(Square((Var(1,18,0) - Var(2,18,1)))))) + (IntCst(1) + (Var(3,1,2) * Square(Sqrt(Sum(Square((Var(1,18,0) - Var(2,18,1))))))))) * Exp((Var(4,1,2) * Sqrt(Sum(Square((Var(1,18,0) - Var(2,18,1)))))))) * Var(5,3320,1)),0)\n",
      "       aliases: Var(0,1,2); Var(1,18,0); Var(2,18,1); Var(3,1,2); Var(4,1,2); Var(5,3320,1); \n",
      "       dtype  : float32\n",
      "... Done.\n",
      "Compiling libKeOpstorch7385e76d34 in /home/jake.gardner/.cache/pykeops-1.1.1//build-libKeOpstorch7385e76d34:\n",
      "       formula: Sum_Reduction(((((Var(0,1,2) * Sqrt(Sum(Square((Var(1,18,0) - Var(2,18,1)))))) + (IntCst(1) + (Var(3,1,2) * Square(Sqrt(Sum(Square((Var(1,18,0) - Var(2,18,1))))))))) * Exp((Var(4,1,2) * Sqrt(Sum(Square((Var(1,18,0) - Var(2,18,1)))))))) * Var(5,1,1)),0)\n",
      "       aliases: Var(0,1,2); Var(1,18,0); Var(2,18,1); Var(3,1,2); Var(4,1,2); Var(5,1,1); \n",
      "       dtype  : float32\n",
      "... Done.\n",
      "Compiling libKeOpstorch97105370ea in /home/jake.gardner/.cache/pykeops-1.1.1//build-libKeOpstorch97105370ea:\n",
      "       formula: Sum_Reduction(((((Var(0,1,2) * Sqrt(Sum(Square((Var(1,18,0) - Var(2,18,1)))))) + (IntCst(1) + (Var(3,1,2) * Square(Sqrt(Sum(Square((Var(1,18,0) - Var(2,18,1))))))))) * Exp((Var(4,1,2) * Sqrt(Sum(Square((Var(1,18,0) - Var(2,18,1)))))))) * Var(5,100,1)),0)\n",
      "       aliases: Var(0,1,2); Var(1,18,0); Var(2,18,1); Var(3,1,2); Var(4,1,2); Var(5,100,1); \n",
      "       dtype  : float32\n",
      "... Done.\n"
     ]
    }
   ],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3651, device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(torch.mean(torch.pow(observed_pred.mean - test_y, 2)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
